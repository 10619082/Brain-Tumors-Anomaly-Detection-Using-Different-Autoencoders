{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nfrom torch.utils.data import Dataset\nimport torch.distributions as dist\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\nimport math\nfrom torch.autograd import grad\n\nfrom glob import glob\nfrom IPython.display import display\nfrom PIL import Image\nimport matplotlib\nimport cv2\nfrom random import *\nimport sklearn.metrics as sk\nimport matplotlib.pyplot as plt\n\nimport abc\nfrom warnings import warn\n\nimport skimage.util as skutil\nimport skimage.transform as skt\nimport sklearn.preprocessing as skp\n\nfrom torch.utils.data import random_split\nfrom torch.utils.data import DataLoader \nfrom sklearn.preprocessing import MinMaxScaler\n\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom copy import deepcopy\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-10T00:05:43.316487Z","iopub.execute_input":"2023-01-10T00:05:43.316899Z","iopub.status.idle":"2023-01-10T00:05:45.053693Z","shell.execute_reply.started":"2023-01-10T00:05:43.316819Z","shell.execute_reply":"2023-01-10T00:05:45.052713Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.055984Z","iopub.execute_input":"2023-01-10T00:05:45.056580Z","iopub.status.idle":"2023-01-10T00:05:45.119918Z","shell.execute_reply.started":"2023-01-10T00:05:45.056540Z","shell.execute_reply":"2023-01-10T00:05:45.118773Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def kl_divergence(mu, logsigma):\n        \"\"\"Compute KL divergence KL(q_i(z)||p(z)) for each q_i in the batch.\n        \n        Args:\n            mu: Means of the q_i distributions, shape [batch_size, latent_dim]\n            logsigma: Logarithm of standard deviations of the q_i distributions,\n                      shape [batch_size, latent_dim]\n        \n        Returns:\n            kl: KL divergence for each of the q_i distributions, shape [batch_size]\n        \"\"\"\n        ##########################################################\n        # YOUR CODE HERE\n        sigma = torch.exp(logsigma)\n        \n        kl = 0.5*(torch.sum(sigma**2 + mu**2 - torch.log(sigma**2) - 1))\n        \n        return kl","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.121608Z","iopub.execute_input":"2023-01-10T00:05:45.122127Z","iopub.status.idle":"2023-01-10T00:05:45.131176Z","shell.execute_reply.started":"2023-01-10T00:05:45.122087Z","shell.execute_reply":"2023-01-10T00:05:45.130255Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class brain_dataset(Dataset):\n                          #[64,64],[80,80],[128,128]\n    def __init__(self,\n                 csv_file,\n                 root_dir,\n                 transform=None):\n        \n        self.df = csv_file\n        self.root_dir = root_dir\n        self.transform = transform\n        \n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        #print('ciao',index)\n        \n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        \n       \n\n        img_name = os.path.join(self.root_dir,\n                                self.df.iloc[idx, 1],   #Healty if train\n                                #da togliere quando passo a altro dataset\n                                str(self.df.iloc[idx, 3]),   #ID_Patient_Folder\n                                str(self.df.iloc[idx, 2]),   #type of image 0 1 2 3\n                                self.df.iloc[idx, 0])   #name of image 000.png\n\n\n        image = Image.open(img_name)\n        \n        if self.transform:\n            image = self.transform(image)\n    \n        res = {'image': image}\n\n        return res","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.132700Z","iopub.execute_input":"2023-01-10T00:05:45.133260Z","iopub.status.idle":"2023-01-10T00:05:45.143467Z","shell.execute_reply.started":"2023-01-10T00:05:45.133221Z","shell.execute_reply":"2023-01-10T00:05:45.142388Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class brain_dataset_final(Dataset):\n                          #[64,64],[80,80],[128,128]\n    def __init__(self,\n                 csv_file,\n                 root_dir,\n                 transform=None):\n        \n        self.df = csv_file\n        self.root_dir = root_dir\n        self.transform = transform\n        \n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        #print('ciao',index)\n        \n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n\n        \n        img_name = os.path.join(self.root_dir,\n                                str(self.df.iloc[idx, 3]), #ID_Patient_Folder\n                                'brainmetshare/metshare/train',\n                                self.df.iloc[idx, 1] + '/id', #Healty if train\n                                str(self.df.iloc[idx, 2]), #type of image 0 1 2 3\n                                self.df.iloc[idx, 0]) #name of image 000.png\n                                \n\n\n        image = Image.open(img_name)\n        \n        if self.transform:\n            image = self.transform(image)\n    \n        res = {'image': image}\n\n        return res","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.147109Z","iopub.execute_input":"2023-01-10T00:05:45.148100Z","iopub.status.idle":"2023-01-10T00:05:45.156810Z","shell.execute_reply.started":"2023-01-10T00:05:45.148066Z","shell.execute_reply":"2023-01-10T00:05:45.156046Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_csv(path):\n        \n        data = []\n        status = next(os.walk(path))[1][-1]\n\n        if status == 'disease':\n            path1 = path + '/' + status\n            for patient_id_folder in sorted(next(os.walk(path1))[1], key=lambda x: int(x.split(\"_\")[-1]) \n                   if (type(x) == str)\n                   else (x) ):\n                for MRI_type in next(os.walk(os.path.join(path1, patient_id_folder)))[1]:\n                    if MRI_type == '2':\n                        path2 = next(os.walk(os.path.join(path1, patient_id_folder)))[0]\n                        for img in sorted(next(os.walk(os.path.join(path2, MRI_type)))[2], key=lambda x: int(x.split(\".\")[0])):\n                            #obj = {'Name': img, 'status': status}\n                            #dizionario\n                            obj = {'Name': img, 'status': status, 'MRI_type': str(MRI_type),\n                                   'patient_id_folder': patient_id_folder}\n                            data.append(obj)\n                            #print(os.path.join(path2, MRI_type, img, patient_id_folder))\n\n            #df.to_csv(path + '/data_disease.csv')\n            \n            path1 = path + '/' + status\n            for patient_id_folder in sorted(next(os.walk(path1))[1], key=lambda x: int(x.split(\"_\")[-1]) \n                   if (type(x) == str)\n                   else (x) ):\n                for MRI_type in next(os.walk(os.path.join(path1, patient_id_folder)))[1]:\n                    if MRI_type == 'seg':\n                        path2 = next(os.walk(os.path.join(path1, patient_id_folder)))[0]\n                        for img in sorted(next(os.walk(os.path.join(path2, MRI_type)))[2], key=lambda x: int(x.split(\".\")[0])):\n                            # obj = {'Name': img, 'status': status}\n                            # dizionario\n                            obj = {'Name': img, 'status': status, 'MRI_type': str(MRI_type),\n                                   'patient_id_folder': patient_id_folder}\n                            data.append(obj)\n                            #print(os.path.join(path2, MRI_type, img, patient_id_folder))\n            \n            status = next(os.walk(path))[1][0]\n\n\n\n\n\n        path1 = path + '/' + status\n        for patient_id_folder in sorted(next(os.walk(path1))[1], key=lambda x: int(x.split(\"_\")[-1]) \n                   if (type(x) == str)\n                   else (x) ):\n            for MRI_type in next(os.walk(os.path.join(path1, patient_id_folder)))[1]:\n                if MRI_type == '2':\n                    path2 =  next(os.walk(os.path.join(path1, patient_id_folder)))[0]\n                    for img in sorted(next(os.walk(os.path.join(path2, MRI_type)))[2], key=lambda x: int(x.split(\".\")[0])):\n                        \n                        obj = {'Name': img, 'status':status, 'MRI_type' : str(MRI_type),\n                               'patient_id_folder':patient_id_folder }\n                        data.append(obj)\n                        #print(os.path.join(path2, MRI_type, img, patient_id_folder))\n\n        return pd.DataFrame(data)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.158290Z","iopub.execute_input":"2023-01-10T00:05:45.159008Z","iopub.status.idle":"2023-01-10T00:05:45.176312Z","shell.execute_reply.started":"2023-01-10T00:05:45.158973Z","shell.execute_reply":"2023-01-10T00:05:45.175415Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def create_csv_final(path = '/kaggle/input/dataset-finale-finale/Dataset_Finale_finale'):\n        data = []\n        \n        for patient_id_folder in sorted(next(os.walk(path))[1], key=lambda x: int(x.split(\"_\")[-1])):\n            path1 = path + '/' + patient_id_folder + '/brainmetshare/metshare/train/healthy/id/2'\n            if 'Mets_052' != patient_id_folder:\n\n                for img in sorted(next(os.walk(path1))[2], key=lambda x: int(x.split(\".\")[0])):\n                \n                    obj = {'Name': img, 'status': 'healthy', 'MRI_type': str(2),\n                                   'patient_id_folder': patient_id_folder}\n                    data.append(obj)\n\n        return pd.DataFrame(data)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.179194Z","iopub.execute_input":"2023-01-10T00:05:45.179561Z","iopub.status.idle":"2023-01-10T00:05:45.189492Z","shell.execute_reply.started":"2023-01-10T00:05:45.179534Z","shell.execute_reply":"2023-01-10T00:05:45.188503Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def create_dataloaders(path,batch_size,trasform,var):\n    \n    train_csv = create_csv(path + '/train')\n    print('Length of training set: ', len(train_csv))\n\n    val_csv = create_csv(path + '/val')\n    print('Length of validation set: ', len(val_csv))\n\n    \n    \n    if var:\n        \n        test_csv = create_csv(path + '/test')\n        print('Length of test set: ', len(test_csv))\n\n        brainTrain = brain_dataset(csv_file= train_csv,\n                                   root_dir=path + '/train',\n                                   transform=trasform)\n\n\n        brainVal = brain_dataset(csv_file=val_csv,\n                                root_dir=path + '/val',\n                                transform=trasform)\n\n\n        brainTest = brain_dataset(csv_file=test_csv,\n                                  root_dir= path + '/test',\n                                  transform=trasform)\n\n\n        trainloader = DataLoader(brainTrain, batch_size)\n        validationloader = DataLoader(brainVal, batch_size)\n        testloader = DataLoader(brainTest, batch_size=batch_size)\n        return trainloader,validationloader,testloader,brainTrain,brainVal,brainTest\n    \n    else:\n        \n        brainTrain = brain_dataset(csv_file= train_csv,\n                                   root_dir=path + '/train',\n                                   transform=trasform)\n\n\n        brainVal = brain_dataset(csv_file=val_csv,\n                                root_dir=path + '/val',\n                                transform=trasform)\n\n\n        trainloader = DataLoader(brainTrain, batch_size)\n        validationloader = DataLoader(brainVal, batch_size)\n        \n        return trainloader,validationloader,validationloader,brainTrain,brainVal,brainVal","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.191029Z","iopub.execute_input":"2023-01-10T00:05:45.191707Z","iopub.status.idle":"2023-01-10T00:05:45.201361Z","shell.execute_reply.started":"2023-01-10T00:05:45.191673Z","shell.execute_reply":"2023-01-10T00:05:45.200214Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Auto encoder convoluzionale\n\nclass Encoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(Encoder, self).__init__()\n        self.linear1 = nn.Linear(256*256, 512)\n        self.linear2 = nn.Linear(512, latent_dims)\n\n    def forward(self, x):\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.linear1(x))\n        return self.linear2(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(Decoder, self).__init__()\n        self.linear1 = nn.Linear(latent_dims, 512)\n        self.linear2 = nn.Linear(512, 256*256)\n\n    def forward(self, z):\n        z = F.relu(self.linear1(z))\n        z = torch.sigmoid(self.linear2(z))\n        return z.reshape((-1, 1, 256, 256))\n    \nclass Autoencoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(Autoencoder, self).__init__()\n        self.encoder = Encoder(latent_dims)\n        self.decoder = Decoder(latent_dims)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        return self.decoder(z)\n    \n    \ndef train(autoencoder, trainloader, trainset, validationloader, valset, epochs):\n    \n    \n    best_val_loss = 99999999\n    best_epoch = 0\n    counter_early = 0\n    counter = 0\n    patience_early_stopping = 150\n    \n    patience_plateu = 100\n    \n    opt = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',\n        factor=0.1, patience=patience_plateu, threshold=0.0001, threshold_mode='abs', verbose = True)\n    \n    for epoch in range(epochs):\n        autoencoder.train()\n        running_loss = 0\n        flag = 0\n        for i, x in tqdm(enumerate(trainloader), total=int(len(trainset) / trainloader.batch_size)):\n            #train\n            x = x['image']\n            x = x.to(device) # GPU\n            opt.zero_grad()\n            x_hat = autoencoder(x)\n            loss = ((x - x_hat)**2).sum()\n            loss.backward()\n            opt.step()\n            running_loss += loss.item() #gradiente\n            counter += 1\n            \n            \n        train_loss = running_loss/counter\n        #val\n        autoencoder.train()\n        running_loss_val = 0.0\n        counter_val = 0\n        with torch.no_grad():\n            for i, x in tqdm(enumerate(validationloader), total=int(len(valset) / validationloader.batch_size)):\n                \n                x = x['image']\n                x = x.to(device) # GPU\n                x_hat = autoencoder(x)\n                loss = ((x - x_hat)**2).sum()\n                running_loss_val += loss.item()\n                counter_val += 1\n                \n                if flag == 0 and epoch % 30  == 0:\n                    \n                    print('Real input')  \n                    a = np.squeeze(x[0].cpu().detach()) * 255                \n                    a[a<0]=0\n                    display(Image.fromarray(np.uint8(a)))\n                    \n                    print('Reconstructed input')  \n                    a = np.squeeze(x_hat[0].cpu().detach()) * 255                \n                    a[a<0]=0\n                    display(Image.fromarray(np.uint8(a)))\n                    \n                    \n                    print('Difference')  \n                    orig = np.array(np.squeeze(x[0].cpu()))\n                    recon = np.array(np.squeeze(x_hat[0].cpu()))\n                    diff = np.absolute(orig - recon)*255\n                    diff[diff<0] = 0\n                    display(Image.fromarray(np.uint8(diff)))\n                    flag = 1\n\n        val_loss = running_loss_val / counter_val\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_weight_par = autoencoder.state_dict()\n            best_model = deepcopy(autoencoder)\n            best_model.load_state_dict(best_weight_par)\n            best_val_loss = val_loss\n            best_epoch = i\n            counter_early = 0\n            best_orig = orig * 255\n            best_recon = recon * 255\n            best_diff = diff\n        else:\n            counter_early += 1\n            if counter_early > patience_early_stopping: break\n    \n            \n        print('LR:',opt.state_dict()['param_groups'][0]['lr'])\n        print('Best val loss:',best_val_loss,' Epoch:',best_epoch,' Counter:',counter_early )\n        print('epoch:{} \\t'.format(i+1),'trainloss:{}'.format(train_loss),'\\t','valloss:{}'.format(val_loss))\n        \n    print('Real input')  \n    display(Image.fromarray(np.uint8(best_orig)))\n\n    print('Reconstructed input')  \n    display(Image.fromarray(np.uint8(best_recon)))\n\n    print('Difference')  \n    display(Image.fromarray(np.uint8(best_diff)))\n\n    return best_model\n\n'''latent_dims = 256\nautoencoder = Autoencoder(latent_dims).to(device) # GPU\n\n#autoencoder = train(autoencoder, trainloader, trainset, validationloader, valset, 1000)\n\nmodel_path = '/kaggle/working/Model'\ncartellaDaVerificare= Path(model_path)\nif not cartellaDaVerificare.is_dir():\n    os.mkdir(model_path)\ntorch.save(autoencoder.state_dict(), model_path + '/' + 'best_model_AE')'''","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.203042Z","iopub.execute_input":"2023-01-10T00:05:45.203761Z","iopub.status.idle":"2023-01-10T00:05:45.239298Z","shell.execute_reply.started":"2023-01-10T00:05:45.203727Z","shell.execute_reply":"2023-01-10T00:05:45.238498Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"latent_dims = 256\\nautoencoder = Autoencoder(latent_dims).to(device) # GPU\\n\\n#autoencoder = train(autoencoder, trainloader, trainset, validationloader, valset, 1000)\\n\\nmodel_path = '/kaggle/working/Model'\\ncartellaDaVerificare= Path(model_path)\\nif not cartellaDaVerificare.is_dir():\\n    os.mkdir(model_path)\\ntorch.save(autoencoder.state_dict(), model_path + '/' + 'best_model_AE')\""},"metadata":{}}]},{"cell_type":"code","source":"#VAE Auto encoder convoluzionale\n\ndef weights_init(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.kaiming_uniform_(m.weight)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(Decoder, self).__init__()\n        self.linear1 = nn.Linear(latent_dims, 512)\n        self.linear2 = nn.Linear(512, 256*256)\n\n\n    def forward(self, z):\n        z = F.relu(self.linear1(z))\n        z = torch.sigmoid(self.linear2(z))\n        return z.reshape((-1, 1, 256, 256))\n\nclass VariationalEncoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(VariationalEncoder, self).__init__()\n        self.linear1 = nn.Linear(256*256, 512)\n\n        self.linear2 = nn.Linear(512, latent_dims)\n\n        self.linear3 = nn.Linear(512, latent_dims)\n        self.linear3.apply(weights_init)\n\n        self.N = torch.distributions.Normal(0, 1)\n        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n        self.N.scale = self.N.scale.cuda()\n        self.kl = 0\n\n    def forward(self, x):\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.linear1(x))\n        mu =  self.linear2(x)\n        sigma = torch.exp(self.linear3(x))\n        z = mu + sigma*self.N.sample(mu.shape)\n        #originale\n        #print(sigma,mu,(sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum())\n        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n        #paper\n        \n        return z\n    \n    \nclass VariationalAutoencoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(VariationalAutoencoder, self).__init__()\n        self.encoder = VariationalEncoder(latent_dims)\n        self.decoder = Decoder(latent_dims)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        return self.decoder(z)\n    \n    \ndef train(autoencoder, trainloader, trainset, validationloader, valset, epochs):\n    \n    best_val_loss = 99999999\n    best_epoch = 0\n    counter_early = 0\n    counter = 0\n    patience_early_stopping = 20\n    \n    patience_plateu = 10\n    \n    opt = torch.optim.Adam(autoencoder.parameters(), lr=0.0001)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',\n        factor=0.1, patience=patience_plateu, threshold=0.0001, threshold_mode='abs', verbose = True)\n    \n    for epoch in range(epochs):\n        autoencoder.train()\n        running_loss = 0\n        flag = 0\n        for i, x in tqdm(enumerate(trainloader), total=int(len(trainset) / trainloader.batch_size)):\n            #train\n            x = x['image']\n            x = x.to(device) # GPU\n            opt.zero_grad()\n            x_hat = autoencoder(x)\n            loss = ((x - x_hat)**2).sum() + autoencoder.encoder.kl\n            loss.backward()\n            opt.step()\n            running_loss += loss.item() #gradiente\n            counter += 1\n            \n            \n        train_loss = running_loss/counter\n        #val\n        autoencoder.train()\n        running_loss_val = 0.0\n        counter_val = 0\n        with torch.no_grad():\n            for i, x in tqdm(enumerate(validationloader), total=int(len(valset) / validationloader.batch_size)):\n                \n                x = x['image']\n                x = x.to(device) # GPU\n                x_hat = autoencoder(x)\n                loss = ((x - x_hat)**2).sum() + autoencoder.encoder.kl\n                running_loss_val += loss.item()\n                counter_val += 1\n                \n                if flag == 0 and epoch % 5  == 0:\n                    \n                    print('Real input')  \n                    a = np.squeeze(x[50].cpu().detach()) * 255                \n                    a[a<0]=0\n                    display(Image.fromarray(np.uint8(a)))\n                    \n                    print('Reconstructed input')  \n                    a = np.squeeze(x_hat[50].cpu().detach()) * 255                \n                    a[a<0]=0\n                    display(Image.fromarray(np.uint8(a)))\n                    \n                    \n                    print('Difference')  \n                    orig = np.array(np.squeeze(x[50].cpu()))\n                    recon = np.array(np.squeeze(x_hat[50].cpu()))\n                    diff = np.absolute(orig - recon)*255\n                    diff[diff<0] = 0\n                    display(Image.fromarray(np.uint8(diff)))\n                    flag = 1\n\n        val_loss = running_loss_val / counter_val\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_weight_par = autoencoder.state_dict()\n            best_model = deepcopy(autoencoder)\n            best_model.load_state_dict(best_weight_par)\n            best_val_loss = val_loss\n            best_epoch = epoch\n            counter_early = 0\n            best_orig = orig * 255\n            best_recon = recon * 255\n            best_diff = diff\n        else:\n            counter_early += 1\n            if counter_early > patience_early_stopping: break\n            \n        print('LR:',opt.state_dict()['param_groups'][0]['lr'])\n        print('Best val loss:',best_val_loss,' Epoch:',best_epoch,' Counter:',counter_early )\n        print('epoch:{} \\t'.format(epoch+1),'trainloss:{}'.format(train_loss),'\\t','valloss:{}'.format(val_loss))\n        \n    print('Real input')  \n    display(Image.fromarray(np.uint8(best_orig)))\n\n    print('Reconstructed input')  \n    display(Image.fromarray(np.uint8(best_recon)))\n\n    print('Difference')  \n    display(Image.fromarray(np.uint8(best_diff)))\n\n    return best_model\n\n'''latent_dims = 256\nautoencoder = VariationalAutoencoder(latent_dims).to(device) # GPU\nautoencoder.apply(weights_init)\n\n#autoencoder = train(autoencoder, trainloader, trainset, validationloader, valset, 100)\n\n\nmodel_path = '/kaggle/working/Model'\n\ncartellaDaVerificare= Path(model_path)\nif not cartellaDaVerificare.is_dir():\n    os.mkdir(model_path)\n    \ntorch.save(autoencoder.state_dict(), model_path + '/' + 'best_model_VAE')'''","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.242221Z","iopub.execute_input":"2023-01-10T00:05:45.242676Z","iopub.status.idle":"2023-01-10T00:05:45.279601Z","shell.execute_reply.started":"2023-01-10T00:05:45.242638Z","shell.execute_reply":"2023-01-10T00:05:45.278345Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"latent_dims = 256\\nautoencoder = VariationalAutoencoder(latent_dims).to(device) # GPU\\nautoencoder.apply(weights_init)\\n\\n#autoencoder = train(autoencoder, trainloader, trainset, validationloader, valset, 100)\\n\\n\\nmodel_path = '/kaggle/working/Model'\\n\\ncartellaDaVerificare= Path(model_path)\\nif not cartellaDaVerificare.is_dir():\\n    os.mkdir(model_path)\\n    \\ntorch.save(autoencoder.state_dict(), model_path + '/' + 'best_model_VAE')\""},"metadata":{}}]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Modello paper 2\n#MODEL\nclass NoOp(nn.Module):\n    def __init__(self, *args, **kwargs):\n        \"\"\"NoOp Pytorch Module.\n        Forwards the given input as is.\n        \"\"\"\n        super(NoOp, self).__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n\nclass ConvModule(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        conv_op=nn.Conv2d,\n        conv_params=None,\n        normalization_op=None,\n        normalization_params=None,\n        activation_op=nn.LeakyReLU,\n        activation_params=None,\n    ):\n        \"\"\"Basic Conv Pytorch Conv Module\n        Has can have a Conv Op, a Normlization Op and a Non Linearity:\n        x = conv(x)\n        x = some_norm(x)\n        x = nonlin(x)\n        Args:\n            in_channels ([int]): [Number on input channels/ feature maps]\n            out_channels ([int]): [Number of ouput channels/ feature maps]\n            conv_op ([torch.nn.Module], optional): [Conv operation]. Defaults to nn.Conv2d.\n            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...)]. Defaults to None.\n            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...)]. Defaults to nn.LeakyReLU.\n            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n        \"\"\"\n\n        super(ConvModule, self).__init__()\n\n        self.conv_params = conv_params\n        if self.conv_params is None:\n            self.conv_params = {}\n        self.activation_params = activation_params\n        if self.activation_params is None:\n            self.activation_params = {}\n        self.normalization_params = normalization_params\n        if self.normalization_params is None:\n            self.normalization_params = {}\n\n        self.conv = None\n        if conv_op is not None and not isinstance(conv_op, str):\n            self.conv = conv_op(in_channels, out_channels, **self.conv_params)\n\n        self.normalization = None\n        if normalization_op is not None and not isinstance(normalization_op, str):\n            self.normalization = normalization_op(out_channels, **self.normalization_params)\n\n        self.activation = None\n        if activation_op is not None and not isinstance(activation_op, str):\n            self.activation = activation_op(**self.activation_params)\n\n    def forward(self, input, conv_add_input=None, normalization_add_input=None, activation_add_input=None):\n\n        x = input\n\n        if self.conv is not None:\n            if conv_add_input is None:\n                x = self.conv(x)\n            else:\n                x = self.conv(x, **conv_add_input)\n\n        if self.normalization is not None:\n            if normalization_add_input is None:\n                x = self.normalization(x)\n            else:\n                x = self.normalization(x, **normalization_add_input)\n\n        if self.activation is not None:\n            if activation_add_input is None:\n                x = self.activation(x)\n            else:\n                x = self.activation(x, **activation_add_input)\n\n        # nn.functional.dropout(x, p=0.95, training=True)\n\n        return x\n\n\nclass ConvBlock(nn.Module):\n    def __init__(\n        self,\n        n_convs: int,\n        n_featmaps: int,\n        conv_op=nn.Conv2d,\n        conv_params=None,\n        normalization_op=nn.BatchNorm2d,\n        normalization_params=None,\n        activation_op=nn.LeakyReLU,\n        activation_params=None,\n    ):\n        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size)\n        Args:\n            n_convs ([type]): [Number of convolutions]\n            n_featmaps ([type]): [Feature map size of the conv]\n            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n        \"\"\"\n\n        super(ConvBlock, self).__init__()\n\n        self.n_featmaps = n_featmaps\n        self.n_convs = n_convs\n        self.conv_params = conv_params\n        if self.conv_params is None:\n            self.conv_params = {}\n\n        self.conv_list = nn.ModuleList()\n\n        for i in range(self.n_convs):\n            conv_layer = ConvModule(\n                n_featmaps,\n                n_featmaps,\n                conv_op=conv_op,\n                conv_params=conv_params,\n                normalization_op=normalization_op,\n                normalization_params=normalization_params,\n                activation_op=activation_op,\n                activation_params=activation_params,\n            )\n            self.conv_list.append(conv_layer)\n\n    def forward(self, input, **frwd_params):\n        x = input\n        for conv_layer in self.conv_list:\n            x = conv_layer(x)\n\n        return x\n\n\nclass ResBlock(nn.Module):\n    def __init__(\n        self,\n        n_convs,\n        n_featmaps,\n        conv_op=nn.Conv2d,\n        conv_params=None,\n        normalization_op=nn.BatchNorm2d,\n        normalization_params=None,\n        activation_op=nn.LeakyReLU,\n        activation_params=None,\n    ):\n        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size) and a skip/ residual connection:\n        x = input\n        x = conv_block(x)\n        out = x + input\n        Args:\n            n_convs ([type]): [Number of convolutions in the conv block]\n            n_featmaps ([type]): [Feature map size of the conv block]\n            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n        \"\"\"\n        super(ResBlock, self).__init__()\n\n        self.n_featmaps = n_featmaps\n        self.n_convs = n_convs\n        self.conv_params = conv_params\n        if self.conv_params is None:\n            self.conv_params = {}\n\n        self.conv_block = ConvBlock(\n            n_featmaps,\n            n_convs,\n            conv_op=conv_op,\n            conv_params=conv_params,\n            normalization_op=normalization_op,\n            normalization_params=normalization_params,\n            activation_op=activation_op,\n            activation_params=activation_params,\n        )\n\n    def forward(self, input, **frwd_params):\n        x = input\n        x = self.conv_block(x)\n\n        out = x + input\n\n        return out\n\n\n# Basic Generator\nclass BasicGenerator(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        z_dim=256,\n        fmap_sizes=(256, 128, 64),\n        upsample_op=nn.ConvTranspose2d,\n        conv_params=None,\n        normalization_op=NoOp,\n        normalization_params=None,\n        activation_op=nn.LeakyReLU,\n        activation_params=None,\n        block_op=NoOp,\n        block_params=None,\n        to_1x1=True,\n    ):\n        \"\"\"Basic configureable Generator/ Decoder.\n        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used.\n        Args:\n            input_size ((int, int, int): Size of the input in format CxHxW): \n            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n                                            int defines the number of feature maps in the layer]. Defaults to (256, 128, 64).\n            upsample_op ([torch.nn.Module], optional): [Upsampling operation used, to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n            to_1x1 (bool, optional): [If Latent dimesion is a z_dim x 1 x 1 vector (True) or if allows spatial resolution not to be 1x1 (z_dim x H x W) (False) ]. Defaults to True.\n        \"\"\"\n\n        super(BasicGenerator, self).__init__()\n\n        if conv_params is None:\n            conv_params = dict(kernel_size=4, stride=2, padding=1, bias=False)\n        if block_op is None:\n            block_op = NoOp\n        if block_params is None:\n            block_params = {}\n\n        n_channels = input_size[0]\n        input_size_ = np.array(input_size[1:])\n\n        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n        elif len(fmap_sizes) < 2:\n            raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n        else:\n            h_size_bot = fmap_sizes[0]\n\n        # We need to know how many layers we will use at the beginning\n        input_size_new = input_size_ // (2 ** len(fmap_sizes))\n        if np.min(input_size_new) < 2 and z_dim is not None:\n            raise AttributeError(\"fmap_sizes to long, one image dimension has already perished\")\n\n        ### Start block\n        start_block = []\n\n        if not to_1x1:\n            kernel_size_start = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n        else:\n            kernel_size_start = input_size_new.tolist()\n\n        if z_dim is not None:\n            self.start = ConvModule(\n                z_dim,\n                h_size_bot,\n                conv_op=upsample_op,\n                conv_params=dict(kernel_size=kernel_size_start, stride=1, padding=0, bias=False),\n                normalization_op=normalization_op,\n                normalization_params=normalization_params,\n                activation_op=activation_op,\n                activation_params=activation_params,\n            )\n\n            input_size_new = input_size_new * 2\n        else:\n            self.start = NoOp()\n\n        ### Middle block (Done until we reach ? x input_size/2 x input_size/2)\n        self.middle_blocks = nn.ModuleList()\n\n        for h_size_top in fmap_sizes[1:]:\n\n            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n\n            self.middle_blocks.append(\n                ConvModule(\n                    h_size_bot,\n                    h_size_top,\n                    conv_op=upsample_op,\n                    conv_params=conv_params,\n                    normalization_op=normalization_op,\n                    normalization_params={},\n                    activation_op=activation_op,\n                    activation_params=activation_params,\n                )\n            )\n\n            h_size_bot = h_size_top\n            input_size_new = input_size_new * 2\n\n        ### End block\n        self.end = ConvModule(\n            h_size_bot,\n            n_channels,\n            conv_op=upsample_op,\n            conv_params=conv_params,\n            normalization_op=None,\n            activation_op=None,\n        )\n\n    def forward(self, inpt, **kwargs):\n        output = self.start(inpt, **kwargs)\n        for middle in self.middle_blocks:\n            output = middle(output, **kwargs)\n        output = self.end(output, **kwargs)\n        return output\n\n\n# Basic Encoder\nclass BasicEncoder(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        z_dim=256,\n        fmap_sizes=(64, 128, 256),\n        conv_op=nn.Conv2d,\n        conv_params=None,\n        normalization_op=NoOp,\n        normalization_params=None,\n        activation_op=nn.LeakyReLU,\n        activation_params=None,\n        block_op=NoOp,\n        block_params=None,\n        to_1x1=True,\n    ):\n        \"\"\"Basic configureable Encoder.\n        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used. \n        Args:\n            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n                                            int defines the number of feature maps in the layer]. Defaults to (64, 128, 256).\n            conv_op ([torch.nn.Module], optional): [Convolutioon operation used to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ]. Defaults to True.\n        \"\"\"\n        super(BasicEncoder, self).__init__()\n\n        if conv_params is None:\n            conv_params = dict(kernel_size=3, stride=2, padding=1, bias=False)\n        if block_op is None:\n            block_op = NoOp\n        if block_params is None:\n            block_params = {}\n\n        n_channels = input_size[0]\n        input_size_new = np.array(input_size[1:])\n\n        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n        # elif len(fmap_sizes) < 2:\n        #     raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n        else:\n            h_size_bot = fmap_sizes[0]\n\n        ### Start block\n        self.start = ConvModule(\n            n_channels,\n            h_size_bot,\n            conv_op=conv_op,\n            conv_params=conv_params,\n            normalization_op=normalization_op,\n            normalization_params={},\n            activation_op=activation_op,\n            activation_params=activation_params,\n        )\n        input_size_new = input_size_new // 2\n\n        ### Middle block (Done until we reach ? x 4 x 4)\n        self.middle_blocks = nn.ModuleList()\n\n        for h_size_top in fmap_sizes[1:]:\n\n            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n\n            self.middle_blocks.append(\n                ConvModule(\n                    h_size_bot,\n                    h_size_top,\n                    conv_op=conv_op,\n                    conv_params=conv_params,\n                    normalization_op=normalization_op,\n                    normalization_params={},\n                    activation_op=activation_op,\n                    activation_params=activation_params,\n                )\n            )\n\n            h_size_bot = h_size_top\n            input_size_new = input_size_new // 2\n\n            if np.min(input_size_new) < 2 and z_dim is not None:\n                raise (\"fmap_sizes to long, one image dimension has already perished\")\n\n        ### End block\n        if not to_1x1:\n            kernel_size_end = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n        else:\n            kernel_size_end = input_size_new.tolist()\n\n        if z_dim is not None:\n            self.end = ConvModule(\n                h_size_bot,\n                z_dim,\n                conv_op=conv_op,\n                conv_params=dict(kernel_size=kernel_size_end, stride=1, padding=0, bias=False),\n                normalization_op=None,\n                activation_op=None,\n            )\n\n            if to_1x1:\n                self.output_size = (z_dim, 1, 1)\n            else:\n                self.output_size = (z_dim, *[i - (j - 1) for i, j in zip(input_size_new, kernel_size_end)])\n        else:\n            self.end = NoOp()\n            self.output_size = input_size_new\n\n    def forward(self, inpt, **kwargs):\n        output = self.start(inpt, **kwargs)\n        for middle in self.middle_blocks:\n            output = middle(output, **kwargs)\n        output = self.end(output, **kwargs)\n        return output\n\nclass VAE(torch.nn.Module):\n    def __init__(\n        self,\n        input_size,\n        z_dim=256,\n        fmap_sizes=(16, 64, 256, 1024),\n        to_1x1=True,\n        conv_op=torch.nn.Conv2d,\n        conv_params=None,\n        tconv_op=torch.nn.ConvTranspose2d,\n        tconv_params=None,\n        normalization_op=None,\n        normalization_params=None,\n        activation_op=torch.nn.LeakyReLU,\n        activation_params=None,\n        block_op=None,\n        block_params=None,\n        *args,\n        **kwargs\n    ):\n        super(VAE, self).__init__()\n\n        input_size_enc = list(input_size)\n        input_size_dec = list(input_size)\n\n        self.enc = BasicEncoder(\n            input_size=input_size_enc,\n            fmap_sizes=fmap_sizes,\n            z_dim=z_dim * 2,\n            conv_op=conv_op,\n            conv_params=conv_params,\n            normalization_op=normalization_op,\n            normalization_params=normalization_params,\n            activation_op=activation_op,\n            activation_params=activation_params,\n            block_op=block_op,\n            block_params=block_params,\n            to_1x1=to_1x1,\n        )\n        self.dec = BasicGenerator(\n            input_size=input_size_dec,\n            fmap_sizes=fmap_sizes[::-1],\n            z_dim=z_dim,\n            upsample_op=tconv_op,\n            conv_params=tconv_params,\n            normalization_op=normalization_op,\n            normalization_params=normalization_params,\n            activation_op=activation_op,\n            activation_params=activation_params,\n            block_op=block_op,\n            block_params=block_params,\n            to_1x1=to_1x1,\n        )\n\n        self.hidden_size = self.enc.output_size\n\n    def forward(self, inpt, sample=True, no_dist=False, **kwargs):\n        y1 = self.enc(inpt, **kwargs)\n\n        mu, log_std = torch.chunk(y1, 2, dim=1)\n        std = torch.exp(log_std)\n        z_dist = dist.Normal(mu, std)\n        if sample:\n            z_sample = z_dist.rsample()\n        else:\n            z_sample = mu\n\n        x_rec = self.dec(z_sample)\n\n        if no_dist:\n            return x_rec\n        else:\n            return x_rec, z_dist\n\n    def encode(self, inpt, **kwargs):\n        enc = self.enc(inpt, **kwargs)\n        mu, log_std = torch.chunk(enc, 2, dim=1)\n        std = torch.exp(log_std)\n        return mu, std\n\n    def decode(self, inpt, **kwargs):\n        x_rec = self.dec(inpt, **kwargs)\n        return x_rec\n\nclass AE(torch.nn.Module):\n    def __init__(\n        self,\n        input_size,\n        z_dim=1024,\n        fmap_sizes=(16, 64, 256, 1024),\n        to_1x1=True,\n        conv_op=torch.nn.Conv2d,\n        conv_params=None,\n        tconv_op=torch.nn.ConvTranspose2d,\n        tconv_params=None,\n        normalization_op=None,\n        normalization_params=None,\n        activation_op=torch.nn.LeakyReLU,\n        activation_params=None,\n        block_op=None,\n        block_params=None,\n        *args,\n        **kwargs\n    ):\n        super(AE, self).__init__()\n\n        input_size_enc = list(input_size)\n        input_size_dec = list(input_size)\n\n        self.enc = BasicEncoder(\n            input_size=input_size_enc,\n            fmap_sizes=fmap_sizes,\n            z_dim=z_dim,\n            conv_op=conv_op,\n            conv_params=conv_params,\n            normalization_op=normalization_op,\n            normalization_params=normalization_params,\n            activation_op=activation_op,\n            activation_params=activation_params,\n            block_op=block_op,\n            block_params=block_params,\n            to_1x1=to_1x1,\n        )\n        self.dec = BasicGenerator(\n            input_size=input_size_dec,\n            fmap_sizes=fmap_sizes[::-1],\n            z_dim=z_dim,\n            upsample_op=tconv_op,\n            conv_params=tconv_params,\n            normalization_op=normalization_op,\n            normalization_params=normalization_params,\n            activation_op=activation_op,\n            activation_params=activation_params,\n            block_op=block_op,\n            block_params=block_params,\n            to_1x1=to_1x1,\n        )\n\n        self.hidden_size = self.enc.output_size\n\n    def forward(self, inpt, **kwargs):\n\n        y1 = self.enc(inpt, **kwargs)\n\n        x_rec = self.dec(y1)\n\n        return x_rec\n\n    def encode(self, inpt, **kwargs):\n        enc = self.enc(inpt, **kwargs)\n        return enc\n\n    def decode(self, inpt, **kwargs):\n        rec = self.dec(inpt, **kwargs)\n        return rec\n    \n    \n    \n    \ndef kl_loss_fn(z_post, sum_samples=True, correct=False, sumdim=(1,2,3)):\n    z_prior = dist.Normal(0, 1.0)\n    kl_div = dist.kl_divergence(z_post, z_prior)\n    if correct:\n        kl_div = torch.sum(kl_div, dim=sumdim)\n    else:\n        kl_div = torch.mean(kl_div, dim=sumdim)\n    if sum_samples:\n        return torch.mean(kl_div)\n    else:\n        return kl_div\n\ndef rec_loss_fn(recon_x, x, sum_samples=True, correct=False, sumdim=(1,2,3)):\n    if correct:\n        x_dist = dist.Laplace(recon_x, 1.0)\n        log_p_x_z = x_dist.log_prob(x)\n        log_p_x_z = torch.sum(log_p_x_z, dim=sumdim)\n    else:\n        log_p_x_z = -torch.abs(recon_x - x)\n        log_p_x_z = torch.mean(log_p_x_z, dim=sumdim)\n    if sum_samples:\n        return -torch.mean(log_p_x_z)\n    else:\n        return -log_p_x_z\n\ndef geco_beta_update(beta, error_ema, goal, step_size, min_clamp=1e-10, max_clamp=1e4, speedup=None):\n    constraint = (error_ema - goal).detach()\n    if speedup is not None and constraint > 0.0:\n        beta = beta * torch.exp(speedup * step_size * constraint)\n    else:\n        beta = beta * torch.exp(step_size * constraint)\n    if min_clamp is not None:\n        beta = np.max((beta.item(), min_clamp))\n    if max_clamp is not None:\n        beta = np.min((beta.item(), max_clamp))\n    return beta\n\ndef get_ema(new, old, alpha):\n    if old is None:\n        return new\n    return (1.0 - alpha) * new + alpha * old\n\nimport random\ndef get_range_val(value, rnd_type=\"uniform\"):\n    if isinstance(value, (list, tuple, np.ndarray)):\n        if len(value) == 2:\n            if value[0] == value[1]:\n                n_val = value[0]\n            else:\n                orig_type = type(value[0])\n                if rnd_type == \"uniform\":\n                    n_val = random.uniform(value[0], value[1])\n                elif rnd_type == \"normal\":\n                    n_val = random.normalvariate(value[0], value[1])\n                n_val = orig_type(n_val)\n        elif len(value) == 1:\n            n_val = value[0]\n        else:\n            raise RuntimeError(\"value must be either a single vlaue or a list/tuple of len 2\")\n        return n_val\n    else:\n        return value\n    \ndef get_square_mask(data_shape, square_size, n_squares, noise_val=(0, 0), channel_wise_n_val=False, square_pos=None):\n    \"\"\"Returns a 'mask' with the same size as the data, where random squares are != 0\n    Args:\n        data_shape ([tensor]): [data_shape to determine the shape of the returned tensor]\n        square_size ([tuple]): [int/ int tuple (min_size, max_size), determining the min and max squear size]\n        n_squares ([type]): [int/ int tuple (min_number, max_number), determining the min and max number of squares]\n        noise_val (tuple, optional): [int/ int tuple (min_val, max_val), determining the min and max value given in the \n                                        squares, which habe the value != 0 ]. Defaults to (0, 0).\n        channel_wise_n_val (bool, optional): [Use a different value for each channel]. Defaults to False.\n        square_pos ([type], optional): [Square position]. Defaults to None.\n    \"\"\"\n\n    def mask_random_square(img_shape, square_size, n_val, channel_wise_n_val=False, square_pos=None):\n        \"\"\"Masks (sets = 0) a random square in an image\"\"\"\n        img_h = img_shape[-2]\n        img_w = img_shape[-1]\n\n        img = np.zeros(img_shape)\n        \n\n        if square_pos is None:\n            w_start = np.random.randint(0, img_w - square_size)\n            h_start = np.random.randint(0, img_h - square_size)\n        else:\n            pos_wh = square_pos[np.random.randint(0, len(square_pos))]\n            w_start = pos_wh[0]\n            h_start = pos_wh[1]\n\n        if img.ndim == 2:\n            rnd_n_val = get_range_val(n_val)\n            img[h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n        elif img.ndim == 3:\n            if channel_wise_n_val:\n                for i in range(img.shape[0]):\n                    rnd_n_val = get_range_val(n_val)\n                    img[i, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n            else:\n                rnd_n_val = get_range_val(n_val)\n                img[:, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n        elif img.ndim == 4:\n            if channel_wise_n_val:\n                for i in range(img.shape[0]):\n                    rnd_n_val = get_range_val(n_val)\n                    img[:, i, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n            else:\n                rnd_n_val = get_range_val(n_val)\n                img[:, :, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n\n        return img\n\n    def mask_random_squares(img_shape, square_size, n_squares, n_val, channel_wise_n_val=False, square_pos=None):\n        \"\"\"Masks a given number of squares in an image\"\"\"\n        img = np.zeros(img_shape)\n        for i in range(n_squares):\n            img = mask_random_square(\n                img_shape, square_size, n_val, channel_wise_n_val=channel_wise_n_val, square_pos=square_pos\n            )\n        return img\n\n    ret_data = np.zeros(data_shape)\n   \n    for sample_idx in range(data_shape[0]):\n        # rnd_n_val = get_range_val(noise_val)\n        \n        rnd_square_size = get_range_val(square_size)\n        rnd_n_squares = get_range_val(n_squares)\n        \n        ret_data[sample_idx] = mask_random_squares(\n            data_shape[1:],\n            square_size=rnd_square_size,\n            n_squares=rnd_n_squares,\n            n_val=noise_val,\n            channel_wise_n_val=channel_wise_n_val,\n            square_pos=square_pos,\n        )\n\n    return ret_data","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.281545Z","iopub.execute_input":"2023-01-10T00:05:45.281889Z","iopub.status.idle":"2023-01-10T00:05:45.363511Z","shell.execute_reply.started":"2023-01-10T00:05:45.281862Z","shell.execute_reply":"2023-01-10T00:05:45.362506Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class VQVAEModel(nn.Module):\n    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n        super(VQVAEModel, self).__init__()\n\n        #first number is the number of channel\n\n        self._encoder = Encoder(1 , num_hiddens,\n                                num_residual_layers, \n                                num_residual_hiddens)\n\n        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, \n                                      out_channels=embedding_dim,\n                                      kernel_size=1, \n                                      stride=1)\n\n        self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n                                           commitment_cost)\n\n        self._decoder = Decoder(embedding_dim,\n                                num_hiddens, #dim input decoder\n                                num_residual_layers, \n                                num_residual_hiddens)\n        self.initialize_weights()\n\n    def forward(self, x):\n\n        z = self._encoder(x)\n        z = self._pre_vq_conv(z)\n        loss, quantized, perplexity, _ = self._vq_vae(z)\n        x_recon = self._decoder(quantized)\n\n        return x_recon, loss, perplexity\n\n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight)\n\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n        super(Decoder, self).__init__()\n        \n        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=num_hiddens,\n                                 kernel_size=3, \n                                 stride=1, padding=1)\n        \n        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n                                             num_hiddens=num_hiddens,\n                                             num_residual_layers=num_residual_layers,\n                                             num_residual_hiddens=num_residual_hiddens)\n        \n        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n                                                out_channels=num_hiddens//2,\n                                                kernel_size=4, \n                                                stride=2, padding=1)\n        \n        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2,\n                                                #occhio\n                                                out_channels=1,\n                                                kernel_size=4, \n                                                stride=2, padding=1)\n\n    def forward(self, inputs):\n        x = self._conv_1(inputs)\n        x = self._residual_stack(x)\n        x = nn.Dropout(0)(x)\n        x = self._conv_trans_1(x)\n        x = F.relu(x)\n        return self._conv_trans_2(x)\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n        super(Encoder, self).__init__()\n\n        #in_channels = 1\n\n        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=num_hiddens//2, #è una divisione senza parte frazionaria\n                                 kernel_size=4,\n                                 stride=2, padding=1)\n\n        #shape [batch,\n        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n                                 out_channels=num_hiddens,\n                                 kernel_size=4,\n                                 stride=2, padding=1)\n        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n                                 out_channels=num_hiddens,\n                                 kernel_size=3,\n                                 stride=1, padding=1)\n        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n                                             num_hiddens=num_hiddens,\n                                             num_residual_layers=num_residual_layers,\n                                             num_residual_hiddens=num_residual_hiddens)\n\n\n    def forward(self, inputs):\n        x = self._conv_1(inputs)\n        x = F.relu(x)\n        x = self._conv_2(x)\n        x = F.relu(x)\n        x = nn.Dropout(0)(x)\n        x = self._conv_3(x)\n        return self._residual_stack(x)\n\nclass ResidualStack(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n        super(ResidualStack, self).__init__()\n        self._num_residual_layers = num_residual_layers\n        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n                             for _ in range(self._num_residual_layers)])\n\n    def forward(self, x):\n        for i in range(self._num_residual_layers):\n            x = self._layers[i](x)\n        return F.relu(x)\n\nclass Residual(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n        super(Residual, self).__init__()\n        self._block = nn.Sequential(\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=num_residual_hiddens,\n                      kernel_size=3, stride=1, padding=1, bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=num_residual_hiddens,\n                      out_channels=num_hiddens,\n                      kernel_size=1, stride=1, bias=False)\n        )\n    \n    def forward(self, x):\n        return x + self._block(x)\n\nclass VectorQuantizer(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n        super(VectorQuantizer, self).__init__()\n        \n        self._embedding_dim = embedding_dim\n        self._num_embeddings = num_embeddings\n        \n        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n        self._commitment_cost = commitment_cost\n\n    def forward(self, inputs):\n        # convert inputs from BCHW -> BHWC\n        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n        input_shape = inputs.shape\n        \n        # Flatten input\n        flat_input = inputs.view(-1, self._embedding_dim)\n        \n        # Calculate distances\n        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n                    + torch.sum(self._embedding.weight**2, dim=1)\n                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n            \n        # Encoding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n        encodings.scatter_(1, encoding_indices, 1)\n        \n        # Quantize and unflatten\n        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n        \n        # Loss\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n        \n        quantized = inputs + (quantized - inputs).detach()\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        # convert quantized from BHWC -> BCHW\n        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.365384Z","iopub.execute_input":"2023-01-10T00:05:45.365775Z","iopub.status.idle":"2023-01-10T00:05:45.399719Z","shell.execute_reply.started":"2023-01-10T00:05:45.365739Z","shell.execute_reply":"2023-01-10T00:05:45.398438Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Training function\ndef train_VQVAE(model, train_loader, validation_loader, num_epochs, optimizer, scaler, device,beta,theta, path_checkpoint,\n         image_path, displ,train_loader_patch,ce_factor):\n    \n    best_val = 9999999999\n    best_epoch = 0\n    use_geco = False\n    vae_loss_ema = 1\n    flag = 0 \n    \n    if ce_factor > 0:\n            \n        for epoch in tqdm(range(num_epochs)):\n            counter = 0\n            loss_tot = 0\n            model.train()\n\n            #print('Epoch ' + str(epoch) + ': Train')\n            for item1, item2 in zip(enumerate(train_loader),enumerate(train_loader_patch)):\n\n                i, data = item1\n                u, data_patch = item2\n                \n                img = data['image']\n                img = img.to(device)\n\n                img_patch = data_patch['image']\n                \n                optimizer.zero_grad()\n\n                ### VAE Part\n                with autocast():\n                    loss_vqae = 0\n                    if ce_factor < 1:\n                        reconstruction, loss, perplexity = model(img)\n                        recon_error = F.mse_loss(reconstruction, img)\n                        loss = recon_error + loss\n                        \n                        loss_vqae = loss\n\n                ### CE Part\n                loss_ce = 0\n                inpt_noisy = img_patch.to(device)\n\n                with autocast():\n                    reconstruction, _, _ = model(inpt_noisy)\n                    #rec_loss_ce = criterion(x_rec_ce,img)\n                    rec_loss_ce = F.mse_loss(reconstruction, img)\n                    loss_ce = rec_loss_ce\n                    loss = (1.0 - ce_factor) * loss_vqae + ce_factor * loss_ce\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                loss_tot += loss.item()\n                counter += 1\n\n\n            loss = round(loss_tot/(len(train_loader)/counter),4) * 1000\n            model.eval()\n            mood_val_loss = 0\n            counter = 0\n            with torch.no_grad():\n                #print('Epoch '+ str(epoch)+ ': Val')\n                for i, data in enumerate(validation_loader):\n\n                    img = data['image']\n                    img = img.to(device)\n                    \n                    reconstruction, loss, perplexity = model(img)\n                    recon_error = F.mse_loss(reconstruction, img)\n                    loss = recon_error + loss\n                    mood_val_loss += loss.item()\n                    counter += 1\n\n            mood_val_loss = round(mood_val_loss/(len(validation_loader)/counter),4) * 1000\n            if mood_val_loss < best_val:\n                best_val = mood_val_loss\n                best_epoch = epoch\n                best_weight_par = model.state_dict()\n\n            '''checkpoint = {\n                'state_dict': model.state_dict()\n                #'optimizer': optimizer.state_dict()\n\n\n            }'''\n            '''if epoch %5 == 0: \n                #torch.save(checkpoint, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n                torch.save(model.state_dict(), os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))'''\n            if epoch % 10 == 0: \n                \n                if flag == 0:\n                    orig = np.squeeze(img[0].cpu().detach()) * 255                \n                    orig = Image.fromarray(np.uint8(orig))\n                    orig.save(image_path + '/Ori_epoch_' + str(epoch) + '.PNG')\n                    flag = 1\n\n                rec = np.squeeze(reconstruction[0].cpu().detach()) * 255                \n                rec[rec<0]=0\n                rec = Image.fromarray(np.uint8(rec))\n                rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n            if epoch % 10 == 0: torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n\n            print(\"Epoch: \", epoch, \", Training loss: \", loss ,\"Validation loss: \" + str(mood_val_loss) )      \n        rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n        torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n        return model\n    else:\n        for epoch in tqdm(range(num_epochs)):\n            \n            counter = 0\n            loss_tot = 0\n            model.train()\n\n            #print('Epoch ' + str(epoch) + ': Train')\n            for i, data in enumerate(train_loader):\n                \n                img = data['image']\n                img = img.to(device)\n\n                optimizer.zero_grad()\n\n                ### VAE Part\n                with autocast():\n                    \n                    reconstruction, loss, perplexity = model(img)\n                    recon_error = F.mse_loss(reconstruction, img)\n                    loss = recon_error + loss\n                        \n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                loss_tot += loss.item()\n                \n                counter += 1\n\n\n            loss = round(loss_tot/(len(train_loader)/counter),4) * 1000\n            model.eval()\n            mood_val_loss = 0\n            counter = 0\n            with torch.no_grad():\n                #print('Epoch '+ str(epoch)+ ': Val')\n                for i, data in enumerate(validation_loader):\n\n                    img = data['image']\n                    img = img.to(device)\n                    \n                    reconstruction, loss, perplexity = model(img)\n                    recon_error = F.mse_loss(reconstruction, img)\n                    loss = recon_error + loss\n                    mood_val_loss += loss.item()\n                    counter += 1\n\n            mood_val_loss = round(mood_val_loss/(len(validation_loader)/counter),4) * 1000\n            if mood_val_loss < best_val:\n                best_val = mood_val_loss\n                best_epoch = epoch\n                best_weight_par = model.state_dict()\n\n            '''checkpoint = {\n                'state_dict': model.state_dict()\n                #'optimizer': optimizer.state_dict()\n\n\n            }'''\n            '''if epoch %5 == 0: \n                #torch.save(checkpoint, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n                torch.save(model.state_dict(), os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))'''\n            \n            if epoch % 10 == 0: \n                \n                if flag == 0:\n                    orig = np.squeeze(img[0].cpu().detach()) * 255                \n                    orig = Image.fromarray(np.uint8(orig))\n                    orig.save(image_path + '/Ori_epoch_' + str(epoch) + '.PNG')\n                    flag = 1\n\n                rec = np.squeeze(reconstruction[0].cpu().detach()) * 255                \n                rec[rec<0]=0\n                rec = Image.fromarray(np.uint8(rec))\n                rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n            display(orig)\n            display(rec)\n            if epoch % 10 == 0: torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n                \n\n            print(\"Epoch: \", epoch, \", Training loss: \", loss ,\"Validation loss: \" + str(mood_val_loss) )      \n        rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n        torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n        return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.403655Z","iopub.execute_input":"2023-01-10T00:05:45.404378Z","iopub.status.idle":"2023-01-10T00:05:45.449747Z","shell.execute_reply.started":"2023-01-10T00:05:45.404342Z","shell.execute_reply":"2023-01-10T00:05:45.448637Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Training function\ndef train_ae(model, train_loader, validation_loader, num_epochs, optimizer, scaler, device,beta,theta, path_checkpoint,\n         image_path, displ,train_loader_patch,batch_size,ce_factor):\n    \n    criterion = nn.MSELoss(reduction='mean')\n    best_val = 9999999999\n    best_epoch = 0\n    use_geco = False\n    vae_loss_ema = 1\n    flag = 0 \n    \n    if ce_factor > 0:\n            \n        for epoch in tqdm(range(num_epochs)):\n            loss_tot = 0\n            model.train()\n\n            #print('Epoch ' + str(epoch) + ': Train')\n            for item1, item2 in zip(enumerate(train_loader),enumerate(train_loader_patch)):\n\n                i, data = item1\n                u, data_patch = item2\n                img = data['image']\n                img = img.to(device)\n\n                img_patch = data_patch['image']\n                optimizer.zero_grad()\n\n\n\n                ### VAE Part\n                with autocast():\n                    loss_vae = 0\n                    if ce_factor < 1:\n                        x_r = model(img)\n                        loss = criterion(img,x_r)\n                        loss_vae = loss\n\n                ### CE Part\n                loss_ce = 0\n                if ce_factor > 0:\n\n                    inpt_noisy = img_patch.to(device)\n\n                    with autocast():\n                        x_rec_ce = model(inpt_noisy)\n                        rec_loss_ce = criterion(x_rec_ce,img)\n                        loss_ce = rec_loss_ce\n                        loss = (1.0 - ce_factor) * loss_vae + ce_factor * loss_ce\n\n                else:\n                    loss = loss_vae\n\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                loss_tot += loss.item()\n\n\n            loss = round(loss_tot/(len(train_loader)/batch_size),4) * 1000\n            model.eval()\n            mood_val_loss = 0\n            with torch.no_grad():\n                #print('Epoch '+ str(epoch)+ ': Val')\n                for i, data in enumerate(validation_loader):\n\n                    img = data['image']\n                    img = img.to(device)\n\n\n                    x_r = model(img)\n                    loss = criterion(img,x_r)\n                    mood_val_loss += loss.item()\n\n            mood_val_loss = round(mood_val_loss/(len(validation_loader)/batch_size),4) * 1000\n            if mood_val_loss < best_val:\n                best_val = mood_val_loss\n                best_epoch = epoch\n                best_weight_par = model.state_dict()\n\n            '''checkpoint = {\n                'state_dict': model.state_dict()\n                #'optimizer': optimizer.state_dict()\n\n\n            }'''\n            '''if epoch %5 == 0: \n                #torch.save(checkpoint, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n                torch.save(model.state_dict(), os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))'''\n            if epoch % 10 == 0: \n                \n                if flag == 0:\n                    orig = np.squeeze(img[0].cpu().detach()) * 255                \n                    orig = Image.fromarray(np.uint8(orig))\n                    orig.save(image_path + '/Ori_epoch_' + str(epoch) + '.PNG')\n                    flag = 1\n\n                rec = np.squeeze(x_r[0].cpu().detach()) * 255                \n                rec[rec<0]=0\n                rec = Image.fromarray(np.uint8(rec))\n                rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n            if epoch % 10 == 0: torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n\n            print(\"Epoch: \", epoch, \", Training loss: \", loss ,\"Validation loss: \" + str(mood_val_loss) )      \n        rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n        torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n        return model\n    else:\n        for epoch in tqdm(range(num_epochs)):\n            \n            loss_tot = 0\n            model.train()\n\n            #print('Epoch ' + str(epoch) + ': Train')\n            for i, data in enumerate(train_loader):\n                \n                img = data['image']\n                img = img.to(device)\n\n                optimizer.zero_grad()\n\n                ### VAE Part\n                with autocast():\n                    loss_vae = 0\n                    if ce_factor < 1:\n                        x_r = model(img)\n                        loss = criterion(img,x_r)\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                loss_tot += loss.item()\n\n\n            loss = round(loss_tot/(len(train_loader)/batch_size),4) * 1000\n            model.eval()\n            mood_val_loss = 0\n            with torch.no_grad():\n                #print('Epoch '+ str(epoch)+ ': Val')\n                for i, data in enumerate(validation_loader):\n\n                    img = data['image']\n                    img = img.to(device)\n\n\n                    x_r = model(img)\n                    loss = criterion(img,x_r)\n                    mood_val_loss += loss.item()\n\n            mood_val_loss = round(mood_val_loss/(len(validation_loader)/batch_size),4) * 1000\n            if mood_val_loss < best_val:\n                best_val = mood_val_loss\n                best_epoch = epoch\n                best_weight_par = model.state_dict()\n\n            '''checkpoint = {\n                'state_dict': model.state_dict()\n                #'optimizer': optimizer.state_dict()\n\n\n            }'''\n            '''if epoch %5 == 0: \n                #torch.save(checkpoint, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n                torch.save(model.state_dict(), os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))'''\n            \n            if epoch % 10 == 0: \n                \n                if flag == 0:\n                    orig = np.squeeze(img[0].cpu().detach()) * 255                \n                    orig = Image.fromarray(np.uint8(orig))\n                    orig.save(image_path + '/Ori_epoch_' + str(epoch) + '.PNG')\n                    flag = 1\n\n                rec = np.squeeze(x_r[0].cpu().detach()) * 255                \n                rec[rec<0]=0\n                rec = Image.fromarray(np.uint8(rec))\n                rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n                \n            if epoch % 10 == 0: torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n\n            print(\"Epoch: \", epoch, \", Training loss: \", loss ,\"Validation loss: \" + str(mood_val_loss) )      \n        rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n        torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n        return model\n        ","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.459571Z","iopub.execute_input":"2023-01-10T00:05:45.460235Z","iopub.status.idle":"2023-01-10T00:05:45.495945Z","shell.execute_reply.started":"2023-01-10T00:05:45.460199Z","shell.execute_reply":"2023-01-10T00:05:45.494927Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Training function\ndef train(model, train_loader, validation_loader, num_epochs, optimizer, scaler, device,beta,theta, path_checkpoint,\n         image_path, displ,train_loader_patch,ce_factor):\n    \n    best_val = 9999999999\n    best_epoch = 0\n    use_geco = False\n    vae_loss_ema = 1\n    flag = 0 \n    \n    \n    if ce_factor > 0:\n        for epoch in tqdm(range(num_epochs)):\n            counter = 0\n            model.train()\n            loss_tot = 0\n            for item1, item2 in zip(enumerate(train_loader),enumerate(train_loader_patch)):\n\n                i, data = item1\n                u, data_patch = item2\n                img = data['image']\n                img = img.to(device)\n\n                img_patch = data_patch['image']\n\n                optimizer.zero_grad()\n                \n                ### VAE Part\n                with autocast():\n                    loss_vae = 0\n                    if ce_factor < 1:\n\n                        x_r, z_dist = model(img)\n                        kl_loss = 0\n                        kl_loss = kl_loss_fn(z_dist, sumdim=(1,2,3)) * beta\n                        rec_loss_vae = rec_loss_fn(x_r, img, sumdim=(1,2,3))\n                        loss_vae = kl_loss + rec_loss_vae * theta\n\n                ### CE Part\n                loss_ce = 0\n\n                inpt_noisy = img_patch.to(device)\n\n                with autocast():\n                    x_rec_ce, _ = model(inpt_noisy)\n                    rec_loss_ce = rec_loss_fn(x_rec_ce, img, sumdim=(1,2,3))\n                    loss_ce = rec_loss_ce\n                    loss = (1.0 - ce_factor) * loss_vae + ce_factor * loss_ce\n                    loss_tot += loss.item()\n\n                if use_geco and ce_factor < 1:\n                    g_goal = 0.1\n                    g_lr = 1e-4\n                    vae_loss_ema = (1.0 - 0.9) * rec_loss_vae + 0.9 * vae_loss_ema\n                    theta = geco_beta_update(theta, vae_loss_ema, g_goal, g_lr, speedup=2)\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                counter += 1\n                \n\n            loss = round(loss_tot/(len(train_loader)/counter),4) * 1000\n            model.eval()\n            mood_val_loss = 0\n            counter = 0\n            with torch.no_grad():\n\n                for i, data in enumerate(validation_loader):\n\n                    img = data['image']\n                    img = img.to(device)\n                    x_r, z_dist = model(img)\n                    kl_loss = 0\n                    kl_loss = kl_loss_fn(z_dist, sumdim=(1,2,3)) * beta\n                    rec_loss_vae = rec_loss_fn(x_r, img, sumdim=(1,2,3))\n                    loss_vae = kl_loss + rec_loss_vae * theta\n                    mood_val_loss += loss_vae.item()\n                    counter += 1\n\n            mood_val_loss = round(mood_val_loss/(len(validation_loader)/counter),4) * 1000\n\n            if mood_val_loss < best_val:\n                best_val = mood_val_loss\n                best_epoch = epoch\n                best_weight_par = model.state_dict()\n\n            '''checkpoint = {\n                'state_dict': model.state_dict()\n                #'optimizer': optimizer.state_dict()\n\n\n            }'''\n            '''if epoch %5 == 0: \n                #torch.save(checkpoint, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n                torch.save(model.state_dict(), os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))'''\n            if epoch % 10 == 0: \n                \n                if flag == 0:\n                    orig = np.squeeze(img[0].cpu().detach()) * 255                \n                    orig = Image.fromarray(np.uint8(orig))\n                    orig.save(image_path + '/Ori_epoch_' + str(epoch) + '.PNG')\n                    flag = 1\n\n                rec = np.squeeze(x_r[0].cpu().detach()) * 255                \n                rec[rec<0]=0\n                rec = Image.fromarray(np.uint8(rec))\n                rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n                \n            if epoch % 10 == 0: torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n\n            print(\"Epoch: \", epoch, \", Training loss: \", str(loss) ,\"Validation loss: \" + str(mood_val_loss))  \n            \n        rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n        torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n        return model\n    else:\n        for epoch in tqdm(range(num_epochs)):\n            counter = 0\n            model.train()\n            loss_tot = 0\n            #print('Epoch ' + str(epoch) + ': Train')\n            for i, data in enumerate(train_loader):\n                \n                img = data['image']\n                img = img.to(device)\n\n                optimizer.zero_grad()\n                \n                ### VAE Part\n                with autocast():\n                    loss_vae = 0\n                    if ce_factor < 1:\n\n                        x_r, z_dist = model(img)\n                        kl_loss = 0\n                        kl_loss = kl_loss_fn(z_dist, sumdim=(1,2,3)) * beta\n                        rec_loss_vae = rec_loss_fn(x_r, img, sumdim=(1,2,3))\n                        loss_vae = kl_loss + rec_loss_vae * theta\n                        \n                loss = loss_vae\n\n                if use_geco and ce_factor < 1:\n                    g_goal = 0.1\n                    g_lr = 1e-4\n                    vae_loss_ema = (1.0 - 0.9) * rec_loss_vae + 0.9 * vae_loss_ema\n                    theta = geco_beta_update(theta, vae_loss_ema, g_goal, g_lr, speedup=2)\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                loss_tot += loss.item()\n                counter += 1\n\n            loss = round(loss_tot/(len(train_loader)/counter),4) * 1000\n            model.eval()\n            mood_val_loss = 0\n            counter = 0\n            with torch.no_grad():\n\n                for i, data in enumerate(validation_loader):\n\n                    img = data['image']\n                    img = img.to(device)\n                    x_r, z_dist = model(img)\n                    kl_loss = 0\n                    kl_loss = kl_loss_fn(z_dist, sumdim=(1,2,3)) * beta\n                    rec_loss_vae = rec_loss_fn(x_r, img, sumdim=(1,2,3))\n                    loss_vae = kl_loss + rec_loss_vae * theta\n                    mood_val_loss += loss_vae.item()\n                    counter += 1\n                        \n\n            mood_val_loss = round(mood_val_loss/(len(validation_loader)/counter),4) * 1000\n\n            if mood_val_loss < best_val:\n                best_val = mood_val_loss\n                best_epoch = epoch\n                best_weight_par = model.state_dict()\n\n            '''checkpoint = {\n                'state_dict': model.state_dict()\n                #'optimizer': optimizer.state_dict()\n\n\n            }'''\n            '''if epoch %5 == 0: \n                #torch.save(checkpoint, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n                torch.save(model.state_dict(), os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))'''\n            if epoch % 10 == 0: \n                \n                if flag == 0:\n                    orig = np.squeeze(img[0].cpu().detach()) * 255                \n                    orig = Image.fromarray(np.uint8(orig))\n                    orig.save(image_path + '/Ori_epoch_' + str(epoch) + '.PNG')\n                    flag = 1\n\n                rec = np.squeeze(x_r[0].cpu().detach()) * 255                \n                rec[rec<0]=0\n                rec = Image.fromarray(np.uint8(rec))\n                rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n                \n                \n                \n            if epoch % 10 == 0: torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))\n\n\n            print(\"Epoch: \", epoch, \", Training loss: \", str(loss) ,\"Validation loss: \" + str(mood_val_loss) )  \n        rec.save(image_path + '/Rec_epoch_' + str(epoch) + '_loss_' + str(mood_val_loss) + '.PNG')\n        torch.save(best_weight_par, os.path.join(path_checkpoint + '/Epoch_' + str(epoch) + \".pt\"))","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:45.497403Z","iopub.execute_input":"2023-01-10T00:05:45.498067Z","iopub.status.idle":"2023-01-10T00:05:45.543637Z","shell.execute_reply.started":"2023-01-10T00:05:45.497991Z","shell.execute_reply":"2023-01-10T00:05:45.542485Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"##### Set model    \nfrom torch.optim import Adam\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.autograd import Variable\n    \ninput_dim = (192,192) # Slice shapes\ninput_size = (1,192,192)\nz_dim = 1024\nmodel_feature_map_sizes=(16, 64, 256, 1024) # Compact vae\n\n#Serve al codice che dice che ho a che fare con imm 2d\nconv = nn.Conv2d\nconvt = nn.ConvTranspose2d\nd = 2\n\n\nmodel = AE(input_size=input_size, z_dim=z_dim, fmap_sizes=model_feature_map_sizes,\n           conv_op=conv,\n           tconv_op=convt,\n           activation_op=torch.nn.PReLU)\n\nmodel.d = d\n\nmodel.to(device)\n\nparam_size = 0\nfor param in model.parameters():\n    param_size += param.nelement() * param.element_size()\nbuffer_size = 0\nfor buffer in model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\nsize_all_mb = (param_size + buffer_size) / 1024**2\nprint('model size: {:.3f}MB'.format(size_all_mb))\n","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:11:07.211908Z","iopub.execute_input":"2023-01-10T00:11:07.212998Z","iopub.status.idle":"2023-01-10T00:11:10.596184Z","shell.execute_reply.started":"2023-01-10T00:11:07.212955Z","shell.execute_reply":"2023-01-10T00:11:10.595143Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"model size: 1178.662MB\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:53.368667Z","iopub.execute_input":"2023-01-10T00:05:53.369276Z","iopub.status.idle":"2023-01-10T00:05:53.373844Z","shell.execute_reply.started":"2023-01-10T00:05:53.369236Z","shell.execute_reply":"2023-01-10T00:05:53.372738Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#path = '/kaggle/input/168-image/Train100ImagesOnly/brainmetshare/metshare'\n#path = '/kaggle/input/brainmetshare/DataSplittedHealthyDiseased/brainmetshare/metshare'\npath = '/kaggle/input/dataset-finale/Dataset_progetto_advanced_deep_learning'\npath_patch = '/kaggle/input/dataset-finale-patch/Dataset_progetto_advanced_deep_learning_patch'\nbatch_size = 64\ninput_image = [192,192]\ntransform = transforms.Compose([\n    transforms.Resize(input_image),\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:53.375491Z","iopub.execute_input":"2023-01-10T00:05:53.375856Z","iopub.status.idle":"2023-01-10T00:05:53.389015Z","shell.execute_reply.started":"2023-01-10T00:05:53.375821Z","shell.execute_reply":"2023-01-10T00:05:53.388147Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"input_channel = 1\n#num_training_updates = 15000\n\nnum_hiddens = 256     #larghezza latent space 512/1024\nnum_residual_hiddens = 4\nnum_residual_layers = 1\n\nembedding_dim = 256     #provo prima questi 3   aumenta prima dimensione\nnum_embeddings = 512\n\ncommitment_cost = 0.25    #più alto + vicino al valore del codebook\n\ndecay = 0.99\n\nlearning_rate = 1e-4\n\n'''# initialize the model\nmodel = VQVAEModel(num_hiddens, num_residual_layers, num_residual_hiddens,\n    num_embeddings, embedding_dim, commitment_cost, decay).to(device)'''\n\nparam_size = 0\nfor param in model.parameters():\n    param_size += param.nelement() * param.element_size()\nbuffer_size = 0\nfor buffer in model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\nsize_all_mb = (param_size + buffer_size) / 1024**2\nprint('model size: {:.3f}MB'.format(size_all_mb))","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:53.392044Z","iopub.execute_input":"2023-01-10T00:05:53.392345Z","iopub.status.idle":"2023-01-10T00:05:53.403157Z","shell.execute_reply.started":"2023-01-10T00:05:53.392321Z","shell.execute_reply":"2023-01-10T00:05:53.402233Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"model size: 1754.662MB\n","output_type":"stream"}]},{"cell_type":"code","source":"lr = 1e-4\noptimizer = Adam(model.parameters(), lr=lr)\nscaler = GradScaler()\n\npath_checkpoint = '/kaggle/working/Checkpoint'\nsave_image_path = '/kaggle/working/Images'\n\n    \ncartellaDaVerificare= Path(path_checkpoint)\nif not cartellaDaVerificare.is_dir():\n    os.mkdir(path_checkpoint)\n\ncartellaDaVerificare= Path(save_image_path)\nif not cartellaDaVerificare.is_dir():\n    os.mkdir(save_image_path)\n    \n    \ndf = create_csv_final()\nindex = int((6813 * 0.8)-1)\ntrain_data = df.iloc[:index,:]\nval_data = df.iloc[index + 1:,:]\n\n\nbrainTrain = brain_dataset_final(csv_file=train_data,\n                                  root_dir= '/kaggle/input/dataset-finale-finale/Dataset_Finale_finale',\n                                  transform=transform)\n\nbrainVal = brain_dataset_final(csv_file=val_data,\n                                  root_dir= '/kaggle/input/dataset-finale-finale/Dataset_Finale_finale',\n                                  transform=transform)\n\ntrainloader = DataLoader(brainTrain, batch_size)\nvalidationloader = DataLoader(brainVal, batch_size)\n\n#train\nbeta = 0.01\ntheta = 1\n\n'''#per CE VAE-AE\n#print('CE')\ntrainloader,validationloader,testloader,trainset,valset,testset = create_dataloaders(path,batch_size,transform,False)\ntrainloader_patch,validationloader_patch,testloader_patch,trainset_patch,valset_patch,testset_patch = create_dataloaders(path_patch,batch_size,transform,False)\n#train(model, trainloader, validationloader, 200, optimizer, scaler, device, beta, theta, path_checkpoint,save_image_path,False,trainloader_patch,batch_size,0.5)\n#train_ae(model, trainloader, validationloader, 200, optimizer, scaler, device, beta, theta, path_checkpoint,save_image_path,False,trainloader_patch,batch_size,0.5)\ntrain_VQVAE(model, trainloader, validationloader, 200, optimizer, scaler, device,beta,theta, path_checkpoint, save_image_path, False,trainloader,0.5)\n'''\n\n#SENZA CE VAE-AE\nprint('NO CE')\n#trainloader,validationloader,testloader,trainset,valset,testset = create_dataloaders(path,batch_size,transform,False)\n#train(model, trainloader, validationloader, 100, optimizer, scaler, device, beta, theta, path_checkpoint,save_image_path,False,trainloader,0)\ntrain_ae(model, trainloader, validationloader, 100, optimizer, scaler, device, beta, theta, path_checkpoint,save_image_path,False,trainloader,batch_size,0)\n#train_VQVAE(model, trainloader,validationloader, 200, optimizer, scaler, device,beta,theta, path_checkpoint, save_image_path, False,trainloader,0)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:11:13.765533Z","iopub.execute_input":"2023-01-10T00:11:13.765895Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"NO CE\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 1/100 [02:03<3:23:12, 123.15s/it]","output_type":"stream"}]},{"cell_type":"code","source":"def train_step(model,trainloader,trainset,optimizer,beta):\n\n    model.train()\n    running_loss = 0.0\n    counter = 0\n\n    for i, data in tqdm(enumerate(trainloader), total=int(len(trainset) / trainloader.batch_size)):\n        counter += 1\n        data = data['image'] \n        data = data.to(device)\n        \n        optimizer.zero_grad()  \n\n        rec_vae,mu,std = model(data)\n        kl_loss = kl_divergence(mu,std)\n        rec_loss_vae = rec_loss_fn(rec_vae,data)\n        loss = rec_loss_vae + kl_loss*beta\n        loss.backward()\n        running_loss += loss.item() #gradiente\n        optimizer.step()\n\n    train_loss = running_loss / (counter * trainloader.batch_size)\n    return train_loss\n\n\ndef validation_step(model,validationloader,valset,beta):\n    \n    model.eval()\n    running_loss = 0.0\n    counter = 0\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(validationloader), total=int(len(valset) / validationloader.batch_size)):\n            \n            counter += 1\n            data = data['image']\n            data = data.to(device)\n\n            rec_vae,mu, std = model(data)\n            kl_loss = kl_divergence(mu,std)\n            rec_loss_vae = rec_loss_fn(rec_vae,data)\n            loss = rec_loss_vae + kl_loss*beta\n\n            running_loss += loss.item()\n\n    val_loss = running_loss / counter\n    return val_loss,data.cpu().detach(),rec_vae.cpu().detach()\n\n\n\n\n\ndef kl_divergence(mu, logsigma):\n        \"\"\"Compute KL divergence KL(q_i(z)||p(z)) for each q_i in the batch.\n        \n        Args:\n            mu: Means of the q_i distributions, shape [batch_size, latent_dim]\n            logsigma: Logarithm of standard deviations of the q_i distributions,\n                      shape [batch_size, latent_dim]\n        \n        Returns:\n            kl: KL divergence for each of the q_i distributions, shape [batch_size]\n        \"\"\"\n        ##########################################################\n        # YOUR CODE HERE\n        sigma = torch.exp(logsigma)\n        \n        kl = 0.5*(torch.sum(sigma**2 + mu**2 - torch.log(sigma**2) - 1))\n        \n        return kl\n    \ndef rec_loss_fn (recon_x,x):\n\n    \"\"\"\n    The function checks the reconstruction loss of image in VAE\n    \"\"\"\n\n    loss_fn = nn.MSELoss()\n    loss = loss_fn(x,recon_x)\n\n    return loss\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:54.952355Z","iopub.status.idle":"2023-01-10T00:05:54.953112Z","shell.execute_reply.started":"2023-01-10T00:05:54.952847Z","shell.execute_reply":"2023-01-10T00:05:54.952873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_vae_papers(trainloader,trainset,validationloader,valset,epochs,beta,patience_early_stopping,\n                       patience_plateu,learning_rate):\n    \n    weight_decay= 0\n\n    z_dim=256\n    h_size=(256, 128, 64)\n    input_size = [1,256,256]\n\n    model = VAE(input_size, h_size, z_dim)\n    model = model.to(device)\n    # model.apply(utils.weights_init)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n    factor=0.1, patience=patience_plateu, threshold=0.0001, threshold_mode='abs', verbose = True)\n    \n    best_val_loss = 99999999\n    best_epoch = 0\n    counter  = 0\n    \n    for i in range(epochs):\n    \n        train_epoch_loss = train_step(model,trainloader,trainset,optimizer,beta)\n        val_epoch_loss,data,rec_vae = validation_step(model,validationloader,valset,beta)\n        scheduler.step(val_epoch_loss)\n\n        if i % 5 == 0:  \n            print('Real input')  \n            a = np.squeeze(data[0].cpu()) * 255                \n            a[a<0]=0\n            display(Image.fromarray(np.uint8(a)))\n\n\n            print('Rec_vae')  \n            a = np.array(np.squeeze(rec_vae[0].cpu())) * 255\n            a[a<0]=0\n            display(Image.fromarray(np.uint8(a)))\n            count = 0\n\n            print('Difference')  \n            orig = np.array(np.squeeze(data[0].cpu()))\n            recon = np.array(np.squeeze(rec_vae[0].cpu()))\n            diff = np.absolute(orig - recon)*255\n            diff[diff<0] = 0\n            display(Image.fromarray(np.uint8(diff)))\n            count = 0\n\n        if val_epoch_loss < best_val_loss:\n            best_weight_par = model.state_dict()\n            best_model = deepcopy(model)\n            best_model.load_state_dict(best_weight_par)\n            best_val_loss = val_epoch_loss\n            best_epoch = i\n            counter = 0\n        else:\n            counter += 1\n            if counter > patience_early_stopping: break\n        if counter == patience_plateu:\n            print('Loading best model at epoch: ',best_epoch, 'with validation loss of:' , best_val_loss* 1000)\n            model = deepcopy(best_model)\n            model.load_state_dict(best_weight_par)\n            counter = 0\n        print('LR:',optimizer.state_dict()['param_groups'][0]['lr'])\n        print('Best val loss:',best_val_loss * 1000,' Epoch:',best_epoch,' Counter:',counter )\n        print('epoch:{} \\t'.format(i+1),'trainloss:{}'.format(train_epoch_loss*1000),'\\t','valloss:{}'.format(val_epoch_loss*1000))\n\n        \n    print('Loading best model at epoch ',best_epoch,' with validation loss of :',best_val_loss * 1000)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:54.954511Z","iopub.status.idle":"2023-01-10T00:05:54.955241Z","shell.execute_reply.started":"2023-01-10T00:05:54.954970Z","shell.execute_reply":"2023-01-10T00:05:54.954993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''model_vae_papers = training_vae_papers(trainloader = trainloader ,trainset = trainset ,validationloader = validationloader,\n                            valset = valset,epochs = 100 ,beta = 1 ,patience_early_stopping = 20,\n                            patience_plateu = 10,learning_rate = 0.0001)\n\n\nmodel_path = '/kaggle/working/Model'\n\ncartellaDaVerificare= Path(model_path)\nif not cartellaDaVerificare.is_dir():\n    os.mkdir(model_path)\n    \ntorch.save(best_model.state_dict(), model_path + '/' + 'best_model_CEVAE_standard')'''","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:54.956557Z","iopub.status.idle":"2023-01-10T00:05:54.957265Z","shell.execute_reply.started":"2023-01-10T00:05:54.957010Z","shell.execute_reply":"2023-01-10T00:05:54.957033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize([192,192]),\n    transforms.ToTensor()\n])\n\ntest_csv  = create_csv('/kaggle/input/dataset-progetto-advanced-test-set/Dataset_progetto_advanced_TEST_SET')\n\nbrainTest = brain_dataset(csv_file=test_csv,\n                                  root_dir= '/kaggle/input/dataset-progetto-advanced-test-set/Dataset_progetto_advanced_TEST_SET',\n                                  transform=transform)\n\ntestloader = DataLoader(brainTest, 1)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:54.958581Z","iopub.status.idle":"2023-01-10T00:05:54.959289Z","shell.execute_reply.started":"2023-01-10T00:05:54.959035Z","shell.execute_reply":"2023-01-10T00:05:54.959058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage import filters\nfrom skimage import morphology\nfrom scipy import ndimage\n\ninput_dim = (192,192) # Slice shapes\ninput_size = (1,192,192)\nz_dim = 512\nmodel_feature_map_sizes=(16, 64, 256, 512) # Compact vae\n\n#Serve al codice che dice che ho a che fare con imm 2d\nconv = nn.Conv2d\nconvt = nn.ConvTranspose2d\n\nmodel = AE(input_size=input_size, z_dim=z_dim, fmap_sizes=model_feature_map_sizes,\n           conv_op=conv,\n           tconv_op=convt,\n           activation_op=torch.nn.PReLU)\n\nmodel.d = d\nmodel.load_state_dict(torch.load('/kaggle/input/vaep2-short-epoch-199/VAEP2_short_Epoch_199.pt'))\nmodel.eval()\n\n\nfor i,data in enumerate(testloader):\n    img = data['image']\n    \n    with autocast():\n            x_r= model(img)\n\n    # Difference of reconstruction and input\n    print(np.shape(x_r))\n    x_r = x_r.float()\n    x_r[0][0][x_r[0][0]< 0] = 0 \n    \n    or_mask = np.copy(img[0][0].detach().numpy())\n    rec_mask = np.copy(x_r[0][0].detach().numpy())\n    \n    or_mask[or_mask>0] = 1\n    rec_mask[rec_mask>0] = 1\n    \n    intersection_mask = or_mask * rec_mask\n    \n    \n    diff_mask = (img.detach().cpu().numpy()- x_r.detach().cpu().numpy()) * intersection_mask\n    \n    display(Image.fromarray(np.uint8(np.squeeze(img.detach().cpu().numpy()) * 255)))\n    display(Image.fromarray(np.uint8(np.squeeze(x_r.detach().cpu().numpy())*255)))\n\n    # Manual Thresholding\n    m_diff_mask = diff_mask.copy()\n    m_diff_mask[m_diff_mask <= 0.25] = 0\n    m_diff_mask[m_diff_mask > 0.25] = 1\n    \n    display(Image.fromarray(np.uint8(np.squeeze(m_diff_mask) * 255)))\n\n    # Otsu Thresholding\n    val = filters.threshold_otsu(m_diff_mask)\n    thr = m_diff_mask > val\n    thr[thr < 0] = 0\n    \n    #display(Image.fromarray(np.uint8(np.squeeze(thr) * 255)))\n\n    # Morphological Opening\n    final = np.zeros_like(thr)\n    for i in range(thr.shape[0]):\n        final[i,0] = torch.tensor(morphology.area_opening(thr[i,0], area_threshold= 20 ))\n    final[img.cpu() == 0] = 0\n    \n    display(Image.fromarray(np.uint8(np.squeeze(final) * 255)))\n\n    '''s_index = 90\n    fig = plt.figure(figsize=(15, 15))\n    ax1 = fig.add_subplot(1,6,1)\n    #ax1.set_title('Input image', fontsize=12)\n    rotated_img = ndimage.rotate(orig_out[s_index, 0], -90)\n    ax1.imshow(rotated_img, cmap='gray')\n    ax1.tick_params(axis='both', which='major', labelsize=4)\n    plt.axis('off')\n\n    ax1 = fig.add_subplot(1,6,1)\n    #ax1.set_title('Input image', fontsize=12)\n    rotated_img = ndimage.rotate(orig_out.cpu().detach().numpy()[s_index, 0], -90)\n    ax1.imshow(rotated_img, cmap='gray')\n    ax1.tick_params(axis='both', which='major', labelsize=4)\n    plt.axis('off')\n\n    ax1 = fig.add_subplot(1,6,2)\n    #ax1.set_title('Input image', fontsize=12)\n    rotated_img = ndimage.rotate(img.cpu().detach().numpy()[s_index, 0], -90)\n    ax1.imshow(rotated_img, cmap='gray')\n    ax1.tick_params(axis='both', which='major', labelsize=4)\n    plt.axis('off')'''","metadata":{"execution":{"iopub.status.busy":"2023-01-10T00:05:54.960614Z","iopub.status.idle":"2023-01-10T00:05:54.961325Z","shell.execute_reply.started":"2023-01-10T00:05:54.961070Z","shell.execute_reply":"2023-01-10T00:05:54.961093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
